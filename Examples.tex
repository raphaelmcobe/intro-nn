\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Neural Net Package Examples},
            pdfauthor={Raphael Cobe},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Neural Net Package Examples}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Raphael Cobe}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{12/2018}


\begin{document}
\maketitle

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"neuralnet"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Going to create a neural network to perform square rooting Type
?neuralnet for more information on the neuralnet library

Generate 50 random numbers uniformly distributed between 0 and 100 And
store them as a dataframe

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{traininginput <-}\StringTok{  }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DataTypeTok{min=}\DecValTok{0}\NormalTok{, }\DataTypeTok{max=}\DecValTok{100}\NormalTok{))}
\NormalTok{trainingoutput <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(traininginput)}
\end{Highlighting}
\end{Shaded}

Column bind the data into one variable

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainingdata <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(traininginput,trainingoutput)}
\KeywordTok{colnames}\NormalTok{(trainingdata) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Input"}\NormalTok{,}\StringTok{"Output"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Train the neural network Going to have 10 hidden layers Threshold is a
numeric value specifying the threshold for the partial derivatives of
the error function as stopping criteria.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{net.sqrt <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(Output}\OperatorTok{~}\NormalTok{Input,trainingdata, }\DataTypeTok{hidden=}\DecValTok{10}\NormalTok{, }\DataTypeTok{threshold=}\FloatTok{0.01}\NormalTok{)}
\KeywordTok{print}\NormalTok{(net.sqrt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $call
## neuralnet(formula = Output ~ Input, data = trainingdata, hidden = 10, 
##     threshold = 0.01)
## 
## $response
##      Output
## 1  4.095487
## 2  3.677612
## 3  8.863338
## 4  9.482282
## 5  9.979196
## 6  9.996541
## 7  9.200869
## 8  2.547884
## 9  5.059873
## 10 5.213152
## 11 6.613625
## 12 1.555674
## 13 7.960157
## 14 5.553397
## 15 7.647699
## 16 4.183634
## 17 7.164056
## 18 5.095019
## 19 4.537366
## 20 5.637232
## 21 9.618515
## 22 7.301636
## 23 8.046755
## 24 1.305051
## 25 5.225709
## 26 5.881855
## 27 8.872180
## 28 7.712201
## 29 8.205109
## 30 5.907053
## 31 9.757592
## 32 6.314420
## 33 9.506635
## 34 3.855183
## 35 7.837811
## 36 3.932584
## 37 7.371684
## 38 9.452813
## 39 3.796454
## 40 4.309515
## 41 9.201602
## 42 9.786414
## 43 3.912386
## 44 5.791824
## 45 3.059325
## 46 7.036124
## 47 4.355092
## 48 5.062061
## 49 8.527006
## 50 5.309464
## 
## $covariate
##                
##  [1,] 16.773015
##  [2,] 13.524830
##  [3,] 78.558768
##  [4,] 89.913671
##  [5,] 99.584360
##  [6,] 99.930837
##  [7,] 84.655988
##  [8,]  6.491714
##  [9,] 25.602312
## [10,] 27.176957
## [11,] 43.740036
## [12,]  2.420122
## [13,] 63.364100
## [14,] 30.840222
## [15,] 58.487295
## [16,] 17.502795
## [17,] 51.323694
## [18,] 25.959224
## [19,] 20.587693
## [20,] 31.778385
## [21,] 92.515834
## [22,] 53.313885
## [23,] 64.750266
## [24,]  1.703158
## [25,] 27.308030
## [26,] 34.596223
## [27,] 78.715583
## [28,] 59.478047
## [29,] 67.323818
## [30,] 34.893275
## [31,] 95.210601
## [32,] 39.871896
## [33,] 90.376115
## [34,] 14.862434
## [35,] 61.431275
## [36,] 15.465220
## [37,] 54.341722
## [38,] 89.355675
## [39,] 14.413064
## [40,] 18.571917
## [41,] 84.669471
## [42,] 95.773902
## [43,] 15.306765
## [44,] 33.545224
## [45,]  9.359467
## [46,] 49.507047
## [47,] 18.966824
## [48,] 25.624461
## [49,] 72.709829
## [50,] 28.190407
## 
## $model.list
## $model.list$response
## [1] "Output"
## 
## $model.list$variables
## [1] "Input"
## 
## 
## $err.fct
## function (x, y) 
## {
##     1/2 * (y - x)^2
## }
## <bytecode: 0x563955da6920>
## <environment: 0x563955d1bdc0>
## attr(,"type")
## [1] "sse"
## 
## $act.fct
## function (x) 
## {
##     1/(1 + exp(-x))
## }
## <bytecode: 0x563955da2568>
## <environment: 0x563955da59c0>
## attr(,"type")
## [1] "logistic"
## 
## $linear.output
## [1] TRUE
## 
## $data
##        Input   Output
## 1  16.773015 4.095487
## 2  13.524830 3.677612
## 3  78.558768 8.863338
## 4  89.913671 9.482282
## 5  99.584360 9.979196
## 6  99.930837 9.996541
## 7  84.655988 9.200869
## 8   6.491714 2.547884
## 9  25.602312 5.059873
## 10 27.176957 5.213152
## 11 43.740036 6.613625
## 12  2.420122 1.555674
## 13 63.364100 7.960157
## 14 30.840222 5.553397
## 15 58.487295 7.647699
## 16 17.502795 4.183634
## 17 51.323694 7.164056
## 18 25.959224 5.095019
## 19 20.587693 4.537366
## 20 31.778385 5.637232
## 21 92.515834 9.618515
## 22 53.313885 7.301636
## 23 64.750266 8.046755
## 24  1.703158 1.305051
## 25 27.308030 5.225709
## 26 34.596223 5.881855
## 27 78.715583 8.872180
## 28 59.478047 7.712201
## 29 67.323818 8.205109
## 30 34.893275 5.907053
## 31 95.210601 9.757592
## 32 39.871896 6.314420
## 33 90.376115 9.506635
## 34 14.862434 3.855183
## 35 61.431275 7.837811
## 36 15.465220 3.932584
## 37 54.341722 7.371684
## 38 89.355675 9.452813
## 39 14.413064 3.796454
## 40 18.571917 4.309515
## 41 84.669471 9.201602
## 42 95.773902 9.786414
## 43 15.306765 3.912386
## 44 33.545224 5.791824
## 45  9.359467 3.059325
## 46 49.507047 7.036124
## 47 18.966824 4.355092
## 48 25.624461 5.062061
## 49 72.709829 8.527006
## 50 28.190407 5.309464
## 
## $exclude
## NULL
## 
## $net.result
## $net.result[[1]]
##           [,1]
##  [1,] 4.096436
##  [2,] 3.678704
##  [3,] 8.869741
##  [4,] 9.492246
##  [5,] 9.961062
##  [6,] 9.976645
##  [7,] 9.212442
##  [8,] 2.551101
##  [9,] 5.054777
## [10,] 5.209789
## [11,] 6.621748
## [12,] 1.554867
## [13,] 7.951466
## [14,] 5.555578
## [15,] 7.639941
## [16,] 4.183610
## [17,] 7.163319
## [18,] 5.090256
## [19,] 4.532864
## [20,] 5.640865
## [21,] 9.624570
## [22,] 7.298476
## [23,] 8.038475
## [24,] 1.305326
## [25,] 5.222517
## [26,] 5.889213
## [27,] 8.878771
## [28,] 7.703958
## [29,] 8.198336
## [30,] 5.914724
## [31,] 9.756937
## [32,] 6.324430
## [33,] 9.516074
## [34,] 3.857280
## [35,] 7.829047
## [36,] 3.934607
## [37,] 7.367380
## [38,] 9.463318
## [39,] 3.798398
## [40,] 4.307856
## [41,] 9.213179
## [42,] 9.783983
## [43,] 3.914458
## [44,] 5.797938
## [45,] 3.054102
## [46,] 7.037722
## [47,] 4.352820
## [48,] 5.056985
## [49,] 8.525859
## [50,] 5.307507
## 
## 
## $weights
## $weights[[1]]
## $weights[[1]][[1]]
##            [,1]        [,2]       [,3]       [,4]        [,5]      [,6]
## [1,] -0.7383932  1.44387323  0.9102701 -1.4635209 -0.16807780 0.1978888
## [2,] -1.9214305 -0.06450449 -1.7286738  0.2794147 -0.04270612 0.4979148
##             [,7]       [,8]        [,9]      [,10]
## [1,] -2.59215423  0.8611832  1.25164905  2.5713043
## [2,]  0.03513933 -1.5970781 -0.08708992 -0.3998341
## 
## $weights[[1]][[2]]
##              [,1]
##  [1,]  1.75440980
##  [2,] -0.08550652
##  [3,] -1.74822851
##  [4,]  1.19344063
##  [5,]  2.35755260
##  [6,] -0.88347071
##  [7,]  1.53956544
##  [8,]  6.08233348
##  [9,] -1.39596258
## [10,] -2.03432649
## [11,]  0.77786518
## 
## 
## 
## $generalized.weights
## $generalized.weights[[1]]
##                [,1]
##  [1,] -0.0095307625
##  [2,] -0.0139349565
##  [3,] -0.0008254421
##  [4,] -0.0006409868
##  [5,] -0.0005052764
##  [6,] -0.0005007819
##  [7,] -0.0007228868
##  [8,] -0.0486530931
##  [9,] -0.0048639601
## [10,] -0.0044321450
## [11,] -0.0020085392
## [12,] -0.3774525093
## [13,] -0.0011398091
## [14,] -0.0036197308
## [15,] -0.0012777040
## [16,] -0.0088644913
## [17,] -0.0015521455
## [18,] -0.0047607435
## [19,] -0.0068094531
## [20,] -0.0034458454
## [21,] -0.0006025547
## [22,] -0.0014645104
## [23,] -0.0011053287
## [24,] -0.9278717633
## [25,] -0.0043987779
## [26,] -0.0029899604
## [27,] -0.0008226903
## [28,] -0.0012472340
## [29,] -0.0010454831
## [30,] -0.0029470916
## [31,] -0.0005642273
## [32,] -0.0023485624
## [33,] -0.0006340554
## [34,] -0.0117800603
## [35,] -0.0011910458
## [36,] -0.0109789675
## [37,] -0.0014230950
## [38,] -0.0006494086
## [39,] -0.0124418893
## [40,] -0.0080323991
## [41,] -0.0007226694
## [42,] -0.0005564071
## [43,] -0.0111805086
## [44,] -0.0031492056
## [45,] -0.0259564139
## [46,] -0.0016419728
## [47,] -0.0077613475
## [48,] -0.0048574564
## [49,] -0.0009334572
## [50,] -0.0041833215
## 
## 
## $startweights
## $startweights[[1]]
## $startweights[[1]][[1]]
##            [,1]      [,2]       [,3]      [,4]        [,5]     [,6]
## [1,] -0.8880351 1.3734009  0.8232831 -1.026743 -0.02894368 1.713893
## [2,] -2.1227327 0.5242907 -1.4220759  2.304441  0.82524823 2.212484
##            [,7]       [,8]       [,9]     [,10]
## [1,] -1.5526620  0.5062044  0.5210958  1.627971
## [2,]  0.7559013 -1.9401327 -0.3436264 -2.059972
## 
## $startweights[[1]][[2]]
##             [,1]
##  [1,] -0.7669188
##  [2,] -3.1676895
##  [3,] -1.4839419
##  [4,]  1.2097310
##  [5,] -0.1424928
##  [6,] -1.0242045
##  [7,] -0.9817632
##  [8,]  0.6490916
##  [9,] -0.4949417
## [10,] -1.1953872
## [11,]  0.2765593
## 
## 
## 
## $result.matrix
##                                 [,1]
## error                   1.183061e-03
## reached.threshold       9.632568e-03
## steps                   3.252000e+03
## Intercept.to.1layhid1  -7.383932e-01
## Input.to.1layhid1      -1.921430e+00
## Intercept.to.1layhid2   1.443873e+00
## Input.to.1layhid2      -6.450449e-02
## Intercept.to.1layhid3   9.102701e-01
## Input.to.1layhid3      -1.728674e+00
## Intercept.to.1layhid4  -1.463521e+00
## Input.to.1layhid4       2.794147e-01
## Intercept.to.1layhid5  -1.680778e-01
## Input.to.1layhid5      -4.270612e-02
## Intercept.to.1layhid6   1.978888e-01
## Input.to.1layhid6       4.979148e-01
## Intercept.to.1layhid7  -2.592154e+00
## Input.to.1layhid7       3.513933e-02
## Intercept.to.1layhid8   8.611832e-01
## Input.to.1layhid8      -1.597078e+00
## Intercept.to.1layhid9   1.251649e+00
## Input.to.1layhid9      -8.708992e-02
## Intercept.to.1layhid10  2.571304e+00
## Input.to.1layhid10     -3.998341e-01
## Intercept.to.Output     1.754410e+00
## 1layhid1.to.Output     -8.550652e-02
## 1layhid2.to.Output     -1.748229e+00
## 1layhid3.to.Output      1.193441e+00
## 1layhid4.to.Output      2.357553e+00
## 1layhid5.to.Output     -8.834707e-01
## 1layhid6.to.Output      1.539565e+00
## 1layhid7.to.Output      6.082333e+00
## 1layhid8.to.Output     -1.395963e+00
## 1layhid9.to.Output     -2.034326e+00
## 1layhid10.to.Output     7.778652e-01
## 
## attr(,"class")
## [1] "nn"
\end{verbatim}

Plot the neural network

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(net.sqrt, }\DataTypeTok{rep =} \StringTok{"best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Examples_files/figure-latex/unnamed-chunk-5-1.pdf}

Test the neural network on some training data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testdata <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{((}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{#Generate some squared numbers}
\NormalTok{net.results <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(net.sqrt, testdata) }\CommentTok{#Run them through the neural network}
\end{Highlighting}
\end{Shaded}

Lets see what properties net.sqrt has

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ls}\NormalTok{(net.results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "net.result" "neurons"
\end{verbatim}

Lets see the results

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(net.results}\OperatorTok{$}\NormalTok{net.result)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]
##  [1,] 1.035118
##  [2,] 2.005680
##  [3,] 2.995127
##  [4,] 4.001716
##  [5,] 4.994439
##  [6,] 6.008677
##  [7,] 7.002251
##  [8,] 7.991461
##  [9,] 9.009079
## [10,] 9.979745
\end{verbatim}

Lets display a better version of the results

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleanoutput <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(testdata,}\KeywordTok{sqrt}\NormalTok{(testdata),}
                     \KeywordTok{as.data.frame}\NormalTok{(net.results}\OperatorTok{$}\NormalTok{net.result))}
\KeywordTok{colnames}\NormalTok{(cleanoutput) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Input"}\NormalTok{,}\StringTok{"Expected Output"}\NormalTok{,}\StringTok{"Neural Net Output"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(cleanoutput)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Input Expected Output Neural Net Output
## 1      1               1          1.035118
## 2      4               2          2.005680
## 3      9               3          2.995127
## 4     16               4          4.001716
## 5     25               5          4.994439
## 6     36               6          6.008677
## 7     49               7          7.002251
## 8     64               8          7.991461
## 9     81               9          9.009079
## 10   100              10          9.979745
\end{verbatim}

\hypertarget{sin-function}{%
\section{\texorpdfstring{\texttt{sin}
function}{sin function}}\label{sin-function}}

Generate random data and the dependent variable

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DataTypeTok{min =} \DecValTok{0}\NormalTok{, }\DataTypeTok{max =} \DecValTok{4}\OperatorTok{*}\NormalTok{pi))}
\NormalTok{y <-}\StringTok{ }\KeywordTok{sin}\NormalTok{(x)}

\NormalTok{data <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(x,y)}
\end{Highlighting}
\end{Shaded}

Create the neural network responsible for the sin function

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(neuralnet)}
\NormalTok{sin.nn <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{data =}\NormalTok{ data, }\DataTypeTok{hidden =} \DecValTok{5}\NormalTok{, }\DataTypeTok{stepmax =} \DecValTok{100000}\NormalTok{, }\DataTypeTok{learningrate =} \FloatTok{10e-6}\NormalTok{,  }
                    \DataTypeTok{act.fct =} \StringTok{'logistic'}\NormalTok{, }\DataTypeTok{err.fct =} \StringTok{'sse'}\NormalTok{, }\DataTypeTok{linear.output =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

Visualize the neural network

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(sin.nn)}
\end{Highlighting}
\end{Shaded}

Generate data for the prediction of the using the neural net;

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testdata<-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DataTypeTok{min=}\DecValTok{0}\NormalTok{, }\DataTypeTok{max=}\NormalTok{(}\DecValTok{4}\OperatorTok{*}\NormalTok{pi)))}
\NormalTok{testdata}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    runif(10, min = 0, max = (4 * pi))
## 1                           6.7673676
## 2                           1.4671763
## 3                           2.5142087
## 4                           4.8603552
## 5                           0.7174377
## 6                           8.6750487
## 7                          12.1843057
## 8                           4.8092177
## 9                           9.8749079
## 10                          3.1508135
\end{verbatim}

Calculate the real value using the \texttt{sin} function

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testdata.result <-}\StringTok{ }\KeywordTok{sin}\NormalTok{(testdata)}
\end{Highlighting}
\end{Shaded}

Make the prediction

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sin.nn.result <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(sin.nn, testdata)}
\NormalTok{sin.nn.result}\OperatorTok{$}\NormalTok{net.result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              [,1]
##  [1,]  0.48803856
##  [2,]  0.98706122
##  [3,]  0.60910913
##  [4,] -0.96923976
##  [5,]  0.65970263
##  [6,]  0.65867238
##  [7,] -0.62349970
##  [8,] -0.97311230
##  [9,] -0.31265458
## [10,] -0.00764799
\end{verbatim}

Compare with the real values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{better <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(testdata, sin.nn.result}\OperatorTok{$}\NormalTok{net.result, testdata.result, (sin.nn.result}\OperatorTok{$}\NormalTok{net.result}\OperatorTok{-}\NormalTok{testdata.result))}
\KeywordTok{colnames}\NormalTok{(better) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Input"}\NormalTok{, }\StringTok{"NN Result"}\NormalTok{, }\StringTok{"Result"}\NormalTok{, }\StringTok{"Error"}\NormalTok{)}

\NormalTok{better}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Input   NN Result       Result        Error
## 1   6.7673676  0.48803856  0.465484759  0.022553797
## 2   1.4671763  0.98706122  0.994636242 -0.007575020
## 3   2.5142087  0.60910913  0.587028903  0.022080229
## 4   4.8603552 -0.96923976 -0.989072950  0.019833192
## 5   0.7174377  0.65970263  0.657456176  0.002246453
## 6   8.6750487  0.65867238  0.681440667 -0.022768292
## 7  12.1843057 -0.62349970 -0.372837314 -0.250662384
## 8   4.8092177 -0.97311230 -0.995315762  0.022203463
## 9   9.8749079 -0.31265458 -0.435082552  0.122427970
## 10  3.1508135 -0.00764799 -0.009220741  0.001572751
\end{verbatim}

Calculate the RMSE:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(Metrics)}
\KeywordTok{rmse}\NormalTok{(better}\OperatorTok{$}\NormalTok{Result, better}\OperatorTok{$}\StringTok{`}\DataTypeTok{NN Result}\StringTok{`}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.08960251
\end{verbatim}

Plot the results:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(x,y)}
\KeywordTok{plot}\NormalTok{(sin, }\DecValTok{0}\NormalTok{, (}\DecValTok{4}\OperatorTok{*}\NormalTok{pi), }\DataTypeTok{add=}\NormalTok{T)}
\NormalTok{x1 <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{4}\OperatorTok{*}\NormalTok{pi, }\DataTypeTok{by=}\FloatTok{0.1}\NormalTok{)}
\KeywordTok{lines}\NormalTok{(x1, }\KeywordTok{compute}\NormalTok{(sin.nn, }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x1))}\OperatorTok{$}\NormalTok{net.result, }\DataTypeTok{col=}\StringTok{"green"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Examples_files/figure-latex/unnamed-chunk-18-1.pdf}

\hypertarget{a-classification-problem}{%
\section{A classification problem}\label{a-classification-problem}}

Using the \texttt{iris} dataset

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(iris)}
\NormalTok{iris.dataset <-}\StringTok{ }\NormalTok{iris}
\end{Highlighting}
\end{Shaded}

Check what is inside the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(iris.dataset)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
\end{verbatim}

Change the dataset so we are able to predict classes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris.dataset}\OperatorTok{$}\NormalTok{setosa <-}\StringTok{ }\NormalTok{iris.dataset}\OperatorTok{$}\NormalTok{Species}\OperatorTok{==}\StringTok{"setosa"}
\NormalTok{iris.dataset}\OperatorTok{$}\NormalTok{virginica =}\StringTok{ }\NormalTok{iris.dataset}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "virginica"}
\NormalTok{iris.dataset}\OperatorTok{$}\NormalTok{versicolor =}\StringTok{ }\NormalTok{iris.dataset}\OperatorTok{$}\NormalTok{Species }\OperatorTok{==}\StringTok{ "versicolor"}
\end{Highlighting}
\end{Shaded}

Separate into train and test data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{nrow}\NormalTok{(iris.dataset), }\DataTypeTok{size =} \KeywordTok{nrow}\NormalTok{(iris)}\OperatorTok{*}\FloatTok{0.5}\NormalTok{)}
\NormalTok{train}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]   9  17  27 112 105 116  20  49  99 124 138  95 118 100  79  46  61
## [18]  45  67 106  36  65  98  32  73 121  69  66  54  29  24 132  38 134
## [35] 141  39   4  23  48  97  92   6  71  84 147 107  12  76  33 102  70
## [52]  64 139 111  47  10  13  63  87  62 122 150 117  93   3 143 135 115
## [69]  14 131  30  50  56 113 119
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iristrain <-}\StringTok{ }\NormalTok{iris.dataset[train,]}
\NormalTok{irisvalid <-}\StringTok{ }\NormalTok{iris.dataset[}\OperatorTok{-}\NormalTok{train,]}
\KeywordTok{print}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(iristrain))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 75
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(irisvalid))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 75
\end{verbatim}

Build the Neural Network for the classification:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nn <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(setosa}\OperatorTok{+}\NormalTok{versicolor}\OperatorTok{+}\NormalTok{virginica }\OperatorTok{~}\StringTok{ }\NormalTok{Sepal.Length }\OperatorTok{+}\StringTok{ }\NormalTok{Sepal.Width, }\DataTypeTok{data=}\NormalTok{iristrain, }\DataTypeTok{hidden=}\DecValTok{3}\NormalTok{, }
                \DataTypeTok{err.fct =} \StringTok{"ce"}\NormalTok{, }\DataTypeTok{linear.output =}\NormalTok{ F, }\DataTypeTok{lifesign =} \StringTok{"minimal"}\NormalTok{, }\DataTypeTok{stepmax =} \DecValTok{1000000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## hidden: 3    thresh: 0.01    rep: 1/1    steps:   40380  error: 48.24897 time: 7.61 secs
\end{verbatim}

Let's check the neural network that we just built

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(nn, }\DataTypeTok{rep=}\StringTok{"best"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Examples_files/figure-latex/unnamed-chunk-25-1.pdf}

Let's try to make the prediction:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{comp <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(nn, irisvalid[}\OperatorTok{-}\DecValTok{3}\OperatorTok{:-}\DecValTok{8}\NormalTok{])}
\NormalTok{pred.weights <-}\StringTok{ }\NormalTok{comp}\OperatorTok{$}\NormalTok{net.result}
\NormalTok{idx <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(pred.weights, }\DecValTok{1}\NormalTok{, which.max)}
\NormalTok{pred <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'setosa'}\NormalTok{, }\StringTok{'versicolor'}\NormalTok{, }\StringTok{'virginica'}\NormalTok{)[idx]}
\KeywordTok{table}\NormalTok{(pred, irisvalid}\OperatorTok{$}\NormalTok{Species)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             
## pred         setosa versicolor virginica
##   setosa         23          1         0
##   versicolor      0         15         5
##   virginica       1         10        20
\end{verbatim}

\hypertarget{and-operation}{%
\section{AND operation}\label{and-operation}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AND <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{),}\DecValTok{1}\NormalTok{)}
\NormalTok{OR <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{binary.data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{expand.grid}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)), AND)}
\KeywordTok{print}\NormalTok{(net <-}\StringTok{ }\KeywordTok{neuralnet}\NormalTok{(AND}\OperatorTok{~}\NormalTok{Var1}\OperatorTok{+}\NormalTok{Var2, binary.data, }\DataTypeTok{hidden=}\DecValTok{0}\NormalTok{, }\DataTypeTok{rep=}\DecValTok{10}\NormalTok{, }\DataTypeTok{err.fct=}\StringTok{"ce"}\NormalTok{, }\DataTypeTok{linear.output=}\OtherTok{FALSE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $call
## neuralnet(formula = AND ~ Var1 + Var2, data = binary.data, hidden = 0, 
##     rep = 10, err.fct = "ce", linear.output = FALSE)
## 
## $response
##   AND
## 1   0
## 2   0
## 3   0
## 4   1
## 
## $covariate
##      Var1 Var2
## [1,]    0    0
## [2,]    1    0
## [3,]    0    1
## [4,]    1    1
## 
## $model.list
## $model.list$response
## [1] "AND"
## 
## $model.list$variables
## [1] "Var1" "Var2"
## 
## 
## $err.fct
## function (x, y) 
## {
##     -(y * log(x) + (1 - y) * log(1 - x))
## }
## <bytecode: 0x563955d1ce98>
## <environment: 0x56395b2c6240>
## attr(,"type")
## [1] "ce"
## 
## $act.fct
## function (x) 
## {
##     1/(1 + exp(-x))
## }
## <bytecode: 0x563955da2568>
## <environment: 0x56395b2c66d8>
## attr(,"type")
## [1] "logistic"
## 
## $linear.output
## [1] FALSE
## 
## $data
##   Var1 Var2 AND
## 1    0    0   0
## 2    1    0   0
## 3    0    1   0
## 4    1    1   1
## 
## $exclude
## NULL
## 
## $net.result
## $net.result[[1]]
##              [,1]
## [1,] 2.639393e-06
## [2,] 1.263503e-02
## [3,] 1.233181e-02
## [4,] 9.837492e-01
## 
## $net.result[[2]]
##              [,1]
## [1,] 4.449090e-06
## [2,] 1.427290e-02
## [3,] 1.507455e-02
## [4,] 9.803191e-01
## 
## $net.result[[3]]
##              [,1]
## [1,] 4.119415e-06
## [2,] 1.413567e-02
## [3,] 1.463890e-02
## [4,] 9.810283e-01
## 
## $net.result[[4]]
##              [,1]
## [1,] 2.702937e-06
## [2,] 1.280331e-02
## [3,] 1.242458e-02
## [4,] 9.837043e-01
## 
## $net.result[[5]]
##              [,1]
## [1,] 3.879536e-06
## [2,] 1.402469e-02
## [3,] 1.402670e-02
## [4,] 9.811888e-01
## 
## $net.result[[6]]
##              [,1]
## [1,] 1.039209e-05
## [2,] 1.893066e-02
## [3,] 1.838469e-02
## [4,] 9.720479e-01
## 
## $net.result[[7]]
##              [,1]
## [1,] 5.716219e-06
## [2,] 1.628906e-02
## [3,] 1.532608e-02
## [4,] 9.783021e-01
## 
## $net.result[[8]]
##              [,1]
## [1,] 9.326076e-06
## [2,] 1.792942e-02
## [3,] 1.817949e-02
## [4,] 9.731521e-01
## 
## $net.result[[9]]
##              [,1]
## [1,] 2.419176e-06
## [2,] 1.188369e-02
## [3,] 1.208425e-02
## [4,] 9.838214e-01
## 
## $net.result[[10]]
##              [,1]
## [1,] 3.999916e-06
## [2,] 1.423956e-02
## [3,] 1.436533e-02
## [4,] 9.813554e-01
## 
## 
## $weights
## $weights[[1]]
## $weights[[1]][[1]]
##            [,1]
## [1,] -12.844959
## [2,]   8.486392
## [3,]   8.461795
## 
## 
## $weights[[2]]
## $weights[[2]][[1]]
##            [,1]
## [1,] -12.322807
## [2,]   8.087790
## [3,]   8.143248
## 
## 
## $weights[[3]]
## $weights[[3]][[1]]
##            [,1]
## [1,] -12.399795
## [2,]   8.154978
## [3,]   8.190469
## 
## 
## $weights[[4]]
## $weights[[4]][[1]]
##            [,1]
## [1,] -12.821169
## [2,]   8.476003
## [3,]   8.445592
## 
## 
## $weights[[5]]
## $weights[[5]][[1]]
##            [,1]
## [1,] -12.459791
## [2,]   8.206979
## [3,]   8.207124
## 
## 
## $weights[[6]]
## $weights[[6]][[1]]
##            [,1]
## [1,] -11.474455
## [2,]   7.526595
## [3,]   7.496774
## 
## 
## $weights[[7]]
## $weights[[7]][[1]]
##            [,1]
## [1,] -12.072197
## [2,]   7.971359
## [3,]   7.909443
## 
## 
## $weights[[8]]
## $weights[[8]][[1]]
##            [,1]
## [1,] -11.582687
## [2,]   7.579467
## [3,]   7.593573
## 
## 
## $weights[[9]]
## $weights[[9]][[1]]
##            [,1]
## [1,] -12.932081
## [2,]   8.511447
## [3,]   8.528386
## 
## 
## $weights[[10]]
## $weights[[10]][[1]]
##            [,1]
## [1,] -12.429233
## [2,]   8.191844
## [3,]   8.200765
## 
## 
## 
## $generalized.weights
## $generalized.weights[[1]]
##          [,1]     [,2]
## [1,] 8.486392 8.461795
## [2,] 8.486392 8.461795
## [3,] 8.486392 8.461795
## [4,] 8.486392 8.461795
## 
## $generalized.weights[[2]]
##         [,1]     [,2]
## [1,] 8.08779 8.143248
## [2,] 8.08779 8.143248
## [3,] 8.08779 8.143248
## [4,] 8.08779 8.143248
## 
## $generalized.weights[[3]]
##          [,1]     [,2]
## [1,] 8.154978 8.190469
## [2,] 8.154978 8.190469
## [3,] 8.154978 8.190469
## [4,] 8.154978 8.190469
## 
## $generalized.weights[[4]]
##          [,1]     [,2]
## [1,] 8.476003 8.445592
## [2,] 8.476003 8.445592
## [3,] 8.476003 8.445592
## [4,] 8.476003 8.445592
## 
## $generalized.weights[[5]]
##          [,1]     [,2]
## [1,] 8.206979 8.207124
## [2,] 8.206979 8.207124
## [3,] 8.206979 8.207124
## [4,] 8.206979 8.207124
## 
## $generalized.weights[[6]]
##          [,1]     [,2]
## [1,] 7.526595 7.496774
## [2,] 7.526595 7.496774
## [3,] 7.526595 7.496774
## [4,] 7.526595 7.496774
## 
## $generalized.weights[[7]]
##          [,1]     [,2]
## [1,] 7.971359 7.909443
## [2,] 7.971359 7.909443
## [3,] 7.971359 7.909443
## [4,] 7.971359 7.909443
## 
## $generalized.weights[[8]]
##          [,1]     [,2]
## [1,] 7.579467 7.593573
## [2,] 7.579467 7.593573
## [3,] 7.579467 7.593573
## [4,] 7.579467 7.593573
## 
## $generalized.weights[[9]]
##          [,1]     [,2]
## [1,] 8.511447 8.528386
## [2,] 8.511447 8.528386
## [3,] 8.511447 8.528386
## [4,] 8.511447 8.528386
## 
## $generalized.weights[[10]]
##          [,1]     [,2]
## [1,] 8.191844 8.200765
## [2,] 8.191844 8.200765
## [3,] 8.191844 8.200765
## [4,] 8.191844 8.200765
## 
## 
## $startweights
## $startweights[[1]]
## $startweights[[1]][[1]]
##             [,1]
## [1,] -0.44495892
## [2,] -0.06600759
## [3,]  0.04099456
## 
## 
## $startweights[[2]]
## $startweights[[2]][[1]]
##            [,1]
## [1,]  0.7771934
## [2,] -0.7014104
## [3,] -0.7775516
## 
## 
## $startweights[[3]]
## $startweights[[3]][[1]]
##            [,1]
## [1,] -0.6313952
## [2,] -1.2290220
## [3,] -1.7251306
## 
## 
## $startweights[[4]]
## $startweights[[4]][[1]]
##             [,1]
## [1,]  0.07883116
## [2,]  1.21112587
## [3,] -0.27520759
## 
## 
## $startweights[[5]]
## $startweights[[5]][[1]]
##            [,1]
## [1,]  1.2402089
## [2,]  0.1493788
## [3,] -0.1820758
## 
## 
## $startweights[[6]]
## $startweights[[6]][[1]]
##           [,1]
## [1,] -2.506055
## [2,]  2.406718
## [3,] -1.008426
## 
## 
## $startweights[[7]]
## $startweights[[7]][[1]]
##            [,1]
## [1,] -0.5721973
## [2,] -0.6758414
## [3,] -0.2745572
## 
## 
## $startweights[[8]]
## $startweights[[8]][[1]]
##            [,1]
## [1,]  0.4173131
## [2,] -0.5729333
## [3,]  0.9736954
## 
## 
## $startweights[[9]]
## $startweights[[9]][[1]]
##            [,1]
## [1,]  0.1047188
## [2,] -1.9409528
## [3,] -2.7872136
## 
## 
## $startweights[[10]]
## $startweights[[10]][[1]]
##            [,1]
## [1,]  1.3707669
## [2,] -0.9657560
## [3,] -0.5235121
## 
## 
## 
## $result.matrix
##                            [,1]          [,2]          [,3]          [,4]
## error               0.041510959   0.049446621   0.048141743   0.041820979
## reached.threshold   0.008718672   0.009671032   0.009806978   0.008934925
## steps             125.000000000 132.000000000 130.000000000 130.000000000
## Intercept.to.AND  -12.844958917 -12.322806573 -12.399795214 -12.821168841
## Var1.to.AND         8.486392410   8.087789583   8.154977965   8.476002994
## Var2.to.AND         8.461794561   8.143248369   8.190469432   8.445592408
##                            [,5]          [,6]          [,7]          [,8]
## error               0.047244231   0.066028494   0.053810373   0.063663097
## reached.threshold   0.009244051   0.009567394   0.009922977   0.009270329
## steps             138.000000000  96.000000000 116.000000000 121.000000000
## Intercept.to.AND  -12.459791126 -11.474455090 -12.072197302 -11.582686851
## Var1.to.AND         8.206978839   7.526595025   7.971358619   7.579466715
## Var2.to.AND         8.207124198   7.496773589   7.909442841   7.593572501
##                            [,9]         [,10]
## error               0.040426078   0.047636073
## reached.threshold   0.007791711   0.009964252
## steps             144.000000000 139.000000000
## Intercept.to.AND  -12.932081154 -12.429233085
## Var1.to.AND         8.511447193   8.191843955
## Var2.to.AND         8.528386445   8.200764998
## 
## attr(,"class")
## [1] "nn"
\end{verbatim}

Now to validate the predictions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{input <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{expand.grid}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)))}
\NormalTok{net.results <-}\StringTok{ }\KeywordTok{compute}\NormalTok{(net, input)}
\KeywordTok{cbind}\NormalTok{(}\KeywordTok{round}\NormalTok{(net.results}\OperatorTok{$}\NormalTok{net.result), AND)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        AND
## [1,] 0   0
## [2,] 0   0
## [3,] 0   0
## [4,] 1   1
\end{verbatim}


\end{document}
